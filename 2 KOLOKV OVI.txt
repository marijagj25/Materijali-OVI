КОЛОКВИУМ 15.01.2024

1. Поврзи 4
 
 1.1 Без користење на калкулатор дискутирај на кој начин ќе процениш колку состојби да се разгранат за да се истражи целото дрво на играта?
     - За да се процени колку состојби ќе се разгранат при пребарувањето на целото дрво, се земаат предвид факторот на разгранување и длабочината на пребарување. Кај Connect4, факторот на разгранување зависи од бројот на колони кои не се полни, односно од бројот на можни потези во дадена состојба. На почетокот на играта факторот е најголем, бидејќи се уште не е наполнета ниту една колона, па со текот на играта, како што се пополнуват колоните, факторот се намалува. Со секое ново ниво на пребарување, бројот на состојби експоненцијално расте, па вкупниот број на состојби што minimax ги разгледува е од ред O(b^d), каде b е факторот на разгранување, а d е длабочината. Поради ваквиот експоненцијален раст, целосното пребарување станува прескапо и ќе одземе премногу време и меморија.
     
 1.2 Што е алфа-бета поткастрување? Зошто го користиме во оваа игра?
     - Алфа-бета поткаструвањето е оптимизација на minimax алгоритамот која служи за намалување на бројот на состојби што се разгледуваат при пребарувањето. Алфа ја претставува најдобрата вредност што досега ја има Max играчот, а бета најдобрата вредност за Min играчот. Кога при пребарувањето ќе се утврди дека одредена гранка не може да доведе до подобар резултат од веќе познатите алфа или бета, таа гранка се отфрла. Доколку таа гранка се разгранува на уште поголем број гранки, како секоја од состојбите во Connect4, со нејзино поткастрување, ефикасно сме скратиле голем број состојби за разгранување што чинат многу време и меморија потребни за извршување, без притоа да се промени конечниот исход. 
     
1.3 Практично, со помош на код, покажи дека користењето на алфа-бета поткастрување се исплати. Дали е подобро да се покаже преку време или преку број на разгранети состојби? Објасни го твојот избор.
    - Исплатливоста на алфа-бета поткаструвањето најдобро се прикажува преку броење на разгранетите состојби, бидејќи времето на извршување може да зависи од хардверот и други надворешни фактори. Со практична имплементација на minimax со и без алфа-бета поткастрување и со броење на бројот на посетени јазли, може јасно да се забележи дека алфа-бета разгледува значително помал број состојби.
    visited = 0
def minimax(graph, node, player):
    global visited
    if node in leafs:
        print(node, end=' ')
        visited = visited+1
        return leafs[node]
    best = plus_inf if player == 'MIN' else minus_inf
    for child in graph.neighbours(node):
        result = minimax(graph, child, other_player(player))
        if player == 'MIN' and result < best:
            best = result
        elif player == 'MAX' and result > best:
            best = result
    return best
    
game_result = minimax(g, 'A', 'MAX')
print(game_result)
print(visited)
 
 istovo treba da se napravi i so alfa beta i da se sporedi brojot
 

1.4 Пример за ограничување на длабочината на која се пребарува алгоритмот (MAX_DEPTH).
Во случај кога за потег е дозволено некое максимално време (пр. 10 секунди), објасни како ќе ја изберам вредноста на константата MAX_DEPTH.
    - Со проба и грешка за да видиме која би била идеалната вредност за максималната длабочина, меѓутоа без да се употреби алфа-бета поткастрување. Ќе пробуваме додека одредената вредност овозможува алгоритамот да заврши за време пократко од ограничувањето.
    
1.5 Предложи функција за проценка на победникот (евристика) за оваа игра. Имплементирај ја во код. Функцијата може да се напише без да ја тестираш со готовата игра од аудиториските вежби. Важно е да се објасни секоја логичка целина од кодот на функцијата која ќе ја напишеш.
    - Да ја процени моменталната состојба на таблата во Connect-4 и да врати нумеричка вредност што покажува кој играч е во подобра позиција, при што се анализираат хоризонталните линии, вертикалните линии и двете дијагонали.
    - def score(self):                     ##Функцијата прима една состојба на таблата 
	    score = 0
	    score += self.x_check()        ##секој ред се анализира посебно, се бараат последователни фигури од ист играч. 2, 3 и 4 поврзани фигури имаат различна тежина
	    score += self.y_check()        ##аналогно истото се прави и за колоните и за дијагоналите 
	    score += self.diag_check()       
	    score += self.diag_check(False)  
	    return score                   ##враќа број кој ја одразува предноста на еден од играчите. Позитивна вредност, Blue играчот е во подобра позиција, негативна вредност, Red играчот е во подобра позиција, вредности ±1000 дефинитивно победа, oваа вредност се користи од minimax кога ќе се достигне максималната длабочина
	    

2. БЕЛО ВИНО 

   import pandas as pd
   df =pd.read_csv('..... .csv')
   df
   i se ispishuva tabela (ova se najv e vekje dadeno)
   
   2.1 Објасни на кој начин ни помага функцијата за ентропија кај дрвата на одлучување.
       - Ентропијата мери колку е нехомогена една распределба на класи. Кај дрвата на одлучување таа помага да се избере карактеристика која најмногу ја намалува ентропијата, односно најдобро го дели множеството на помали, похомогени подмножества. Поголема ентропија, понехомогено множество, полошо распределено и обратно. Со пресметка на ентропијата пред и по поделбата на множеството, добиваме информациска придобивка која ни кажува дали е избрана добра карактеристика. Поголема информациска придобивка, подобра карактеристика.
   

   2.2 Подели го податочното множество на два дела. Поголемиот дел нека е 75% и ќе служи за тренирање, а помалиот дел од 25% ќе служи за проверка на поведението на алгоритмите кои ќе ги имплементираш понатаму.
      - df_y = df['quality'] (овде во наводниците треба да стои класата според која го делиме множеството)
	df_x = df.iloc[:, :-1]
	
	from sklearn.model_selection import train_test_split
	
	df_train_x, df_test_x, df_train_y, df_test_y = train_test_split(df_x, df_y, test_size=0.25, random_state=84)
	df_test_x


   2.3 Исцртајте ги на график вината така што на x оската ќе биде ph вредноста, на y оската ќе биде процентот на алкохол, а квалитетот на вината ќе биде претставен со ќе бидат со боја или симбол.
Совет: За помош може да го искористите третиот пример Setting size and color with column names од документацијата на плотли.
       - import plotly.express as px
	 px.scatter(df, x="pH", y="alchohol", color="quality")
	  
	 ако не сака вака, треба да се напише figure=px.scatter(df, x="pH", y="alchohol", color="quality") и после figure.show()
	 

    2.4 Користејќи ја библиотеката sklearn, вчитајте класификатор - дрво за одлучување, претставен преку класата DecisionTreeClassifier, а потоа вметнете ги податоците за тренирање од вториот чекор за да ја истренирате мрежата.
        - from sklearn.tree import DecisionTreeClassifier
          
          clf = DecisionTreeClassifier(criterion='entropy')
	  clf.fit(df_train_x, df_train_y)
	  
   
    2.5 Пресметајте ја прецизноста на алгоритамот за податоците кои тој ги нема видено (на кои нема тренирано). Потоа пресметајте ја прецизноста на алгоритамот за податоците кои ги има видено (на кои има тренирано). Споредете ги резултатите. Ви изгледаат ли во ред?
        - df_pred_y = clf.predict(df_test_x)
	  df_pred_y
	 
	 from sklearn.metrics import accuracy_score
         accuracy_score(df_test_y, df_pred_y)
         
         accuracy_score(df_train_y, clf.predict(df_train_x)) , nekakov komentar vo zavisnost od brojchinjata shto kje se dobijat
	 

    2.6 Кои вина алгоритамот успева да ги распознава подобро, висок процент на алкохол (над 10,5%) или низок процент на алкохол (под 10,5%)?
        - df_high_percentage = df[df['alcohol'] > 10.5]
	  df_low_percentage = df[df['alcohol'] <= 10.5]
	  df_high_percentage

          df_high_percentage_y = df_high_percentage['quality']
          df_high_percentage_x = df_high_percentage.iloc[:, :-1]
          accuracy_score(df_high_percentage_y, clf.predict(df_high_percentage_x))
          
          df_low_percentage_y = df_low_percentage['quality']
          df_low_percentage_x = df_low_percentage.iloc[:, :-1]
          accuracy_score(df_low_percentage_y, clf.predict(df_low_percentage_x))
          
     2.7 Зошто библиотеките за машинско учење кои нудат имплементација на алгоритам за дрва за одлучување, користат gini index наместо ентропија?
         - Gini impurity - функција многу слична на ентропијата. Изгледа слично и самата функција е без употреба на log. Користи множење кое е многу побрзо од пресметувањето на log, а во големи дрва на одлучување каде функцијата многу пати се повикува, многу време ќе се заштеди ако не се пресметува log.
         
     2.8 Колку се квалитетни вината од податочното множество winequality-white-test.csv?
         - df_new = pd.read_csv('winequality-white-test.csv')
	   df_new
	   
	   clf.predict(df_new)
	   
     2.9 Исцртај го дрвото на играта, до длабочина 3? Кој признак најдобро го дели множеството при првата поделба?
         - clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)
 	  clf.fit(df_train_x, df_train_y)
 	  
 	 - со или без max_depth, сепак ќе биде изберена истата карактеристика затоа што ID3 е алчен алгоритам и гледа во моментот која карактеристика за моменталната поделба а max_depth само кажува колку поделби да направи, во овој случај вика по 3тата поделба да престане со работа. Со овој код, може да се провери која е таа карактеристика: 
 	 
 	   from sklearn import tree
           import matplotlib.pyplot as plt
           plt.figure(figsize=(12,8))
           tree.plot_tree(clf, feature_names = df_x.columns, filled=True)
           plt.show()
           
         - јазолот на врвот е победничката карактеристика 
         
     2.10 Имплементирај уште еден алгоритам по твој избор, кој ќе го предвидува квалитетот на вината. Толкувај дали тој алгоритам е подобар од алгоритамот за дрва за одлучување.
          - import pandas as pd
	    from sklearn.model_selection import train_test_split
	    from sklearn.naive_bayes import GaussianNB
	    from sklearn.metrics import accuracy_score
	    
	    data = pd.read_csv("winequality.csv")
	    
	    X = data.drop("quality", axis=1)
	    y = data["quality"]
	    
	    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=84)
	    
	    nb = GaussianNB()
	    nb.fit(X_train, y_train)
	    
	    y_pred = nb.predict(X_test)
	    
	    accuracy = accuracy_score(y_test, y_pred)
	    print("Accuracy (Naive Bayes):", accuracy)
          
     2.11 Дали постапуваме правилно ако за нашите податоци имплементираме многу различни алгоритми, на пример над 20, и го избереме најдобриот? Дали сигурно сме го избрале најдобриот или има нешто труло во оваа постапка? Објасни.
          - Не е правилно вака да се направи затоа што така се претерува со учењето. Ни треба алгоритам што ги генерализира работите/знаењето а не некој што перфектно ги погодува веќе видените податоци и ги меморизирал "на памет".

КОЛОКВИУМ 17.01.2024

1.Минимакс
   Го разгледуваме минимакс со алфа-бета поткастрување.

     1.1 Објасни како работи алфа-бета поткаструвањето. Објасни кои се предностите и недостатоците од имплементацијата на алфа-бета поткаструвањето.
         - Работи на таков начин што, со бележење на алфа (најдобрат потег за Max до тој момент) и бета (најдобар потег за Min до тој момент), постои случај каде што ќе заклучиме дека некоја гранка нема потреба да ја разгрануваме понатаму и ќе застанеме до таму, нема да продолжиме да ја разгрануваме гранката. Предност е тоа што со кратењето на гранки, заштедуваме на време и меморија што алгоритамот ќе ги потроши при анализа на тие гранки. Недостаток е тоа што има поголема комплексност во кодот.
         - Ако се побара за меморијата да се докаже дека се троши со разгранување и дк може да се преполни функцискиот stack:
           def r(x)
               if x=199999999999999:
                  print('da')
               else:
                  r(x+1)

           r(1)


     1.2 Кога играч во дадена игра е ограничен со време за секој потег, едно решение е да поставиме фиксна максимална длабочина, MAX_DEPTH, во самата имплементација на минимакс алгоритамот. Меѓутоа, ова решение не е баш најдобро бидејќи: 
         1. на почетокот на играта лесно може да истече времето додека минимакс пребарува.
         2. на крајот на играта останува многу време, време кое може да се потроши ефективно така што алгоритамот ќе пребарува подлабоко.
Предложете како да се решат овие проблеми, па имплементирајте ги во пајтон. Слободно можете да го имплементирате решението врз веќе постоечки код од лабс, на пр. играта Поврзи 4.
 
         - Овој проблем може да се реши со проба и грешка за да видиме која би била идеалната вредност за максималната длабочина, меѓутоа без да се употреби алфа-бета поткастрување. Ќе пробуваме додека одредената вредност овозможува алгоритамот да заврши за време пократко од ограничувањето.
	 - на таа од лабс во минмакс треба да се смени max_depth = 100
           се копира:
           def get_move_AI(state, player):
    	   MM = player['MM']
           import time
           for n in range(1, 15):
                s = time.time()
    	   	result, move = minimax(state, MM, max_depth = n)
                e = time.time()
                time_ellapsed = (e-s)
                print(n, time_ellapsed)
    	   return move


     1.3 Зошто ни треба функција за проценка на победникот (евристика) кај имплементација на минимакс кај шахот?
	 - Секој минимакс мора да има функција за евристика за алгоритамот да не пребарува до бесконечност кај игри како шахот кои имаат многу состојби за разгранување.

     1.4 Еден начин на кој луѓето размислуваат додека играат шах е да се фокусираат на веројатните/логичките/реалните состојби, а да ги занемаруваат нелогичните потези (жртвување кралица за да се постигне предност подоцна), иако постои можност овие потези да донесат предност во остатокот од играта. Минимакс алгоритамот, каков што го знаеме од предавања и вежби, не размислува вака. Објаснете што би промениле за минимакс да ја добие способноста на луѓето како што е опишана тука. Дали всушност ќе добиеме подобрување или ова, сепак, е уназадување на алгоритамот?
         - Бидејќи пресметуваме предност што се добива/губи со секој направен потег на едниот играч во однос на другиот. Да поставиме некоја вредност која ќе ни претставува сигурна победа и доколку се достигне таа вредност можеме да завршиме со пребарувањето и да кажеме дека играчот сигурно ќе победи/изгуби. Ова е уназадување затоа што ние не го гледаме крајот на играта и нешто што во моментот може да ни изгледа како предност, може понатаму да не донесе до пораз. Не мора ништо да се промени во кодот. 


2. Зрна ориз
     
     2.1 Зошто ID3 е алчен алгоритам? 
         - Алгоритамот е алчен бидејќи секогаш ја одбира најдобрата поделба само за моменталната распределба на податоците, а не ја гледа најдобрата можност генерално за целото дрво.

     2.2 Подели го податочното множество на два дела. Поголемиот дел нека е 75% и ќе служи за тренирање, а помалиот дел од 25% ќе служи за проверка на поведението на алгоритмите кои ќе ги имплементираш понатаму.
      - df_y = df['class'] (овде во наводниците треба да стои класата според која го делиме множеството)
 	df_x = df.iloc[:, :-1]
 
 	from sklearn.model_selection import train_test_split
 
 	df_train_x, df_test_x, df_train_y, df_test_y = train_test_split(df_x, df_y, test_size=0.25, random_state=84)
 	df_test_x

     2.3 Исцртајте ги на график вината така што на x оската ќе биде Area, на y оската ќе биде Perimeter, а видот на зрната ќе биде претставен со боја или симбол.
Совет: За помош може да го искористите третиот пример Setting size and color with column names од документацијата на плотли.
       - import plotly.express as px
 	 px.scatter(df, x="Area", y="Perimeter", color="class")

     2.4 Користејќи ја библиотеката sklearn, вчитајте класификатор - дрво за одлучување, претставен преку класата DecisionTreeClassifier, а потоа вметнете ги податоците за тренирање од вториот чекор за да ја истренирате мрежата.
        - from sklearn.tree import DecisionTreeClassifier
 
          clf = DecisionTreeClassifier(criterion='entropy')
 	  clf.fit(df_train_x, df_train_y)

     2.5 Пресметајте ја прецизноста на алгоритамот за податоците кои тој ги нема видено (на кои нема тренирано). Потоа пресметајте ја прецизноста на алгоритамот за податоците кои ги има видено (на кои има тренирано). Споредете ги резултатите. Ви изгледаат ли во ред?
        - df_pred_y = clf.predict(df_test_x)
 	  df_pred_y
 
 	 from sklearn.metrics import accuracy_score
         accuracy_score(df_test_y, df_pred_y)
 
         accuracy_score(df_train_y, clf.predict(df_train_x))

     
     2.6 Кои зрна алгоритамот успева да ги распознава подобро, Area над просекот или Area под просекот?
         - df.query('Area > 12 000')
	   df['Area'].mean() za da se najde prosekot
    
         - или може вака 
	   df.query(f'Area > {df["Area"].mean()}')
           или
           big = df['Area']>df["Area"].mean()
           small = df['Area'] <= df["Area"].mean()

           big_y = big['class']  	
           big_x = big.iloc[:, :-1]
           small_y = small['class']
           small_x = small.iloc[:, :-1]
 
           print('Accuracy Score on big_x data: ', accuracy_score(y_true=big_y, y_pred=clf.predict(big_x)))
           print('Accuracy Score on small_x data: ', accuracy_score(y_true=small_y, y_pred=small_x))

      2.7 Од кој вид се зрната од податочното множество rice-test.csv?
          - df_new = pd.read_csv('rice-test.csv')
 	    df_new
 
 	    clf.predict(df_new)

     2.8 Што ако податоците се непотполни, недостасуваат Area и Parameter? Нема да може да предвидиме не носи поени.
         - Доколку имаме време или ако се исплати да го истренираме одново алгоритамот без овие колони, да се направи тоа. Ако не, треба да најдеме начин како да се споредат непотполните податоци, со тие што веќе ни се класифицирани и некако според тоа да се класифицираат непотполните податоци.

     2.9 Дали постапуваме правилно ако за нашите податоци имплементираме многу различни алгоритми, на пример над 20, и го избереме најдобриот? Дали сигурно сме го избрале најдобриот или има нешто труло во оваа постапка? Објасни.
          - Не е правилно вака да се направи затоа што така се претерува со учењето. Ни треба алгоритам што ги генерализира работите/знаењето а не некој што перфектно ги погодува веќе видените податоци и ги меморизирал "на памет".

     2.10 Наведи 3 причини зошто би го избрал овој алгоритам наместо другите што ги користевме на лабораториски и часовите.
          1. Подобро работи со помалку податоци, побрзо треба да се истренира
          2. Полесно е за визуелизација
          3. Лесно го дели множеството
     
     2.11 Класата DecisionTreeClassifier има аргумент max_depth. Објасни на што влијае овој аргумент и објасни ја важноста на истиот.
         - Влијае на комплексноста на моделот. За помала вредност на max_depth имаме попросто, поедноставно дрво. За поголема вредност, дрвото е покомплексно. Со овој параметар можеме да спречиме overfitting на дрвото затоа што ако го нема овој параметар, дрвото ќе расте додека не ги класифицира сите податоци перфектно односно ќе преучи и ќе направи overfitting. Ова може да се случи и доколку поставиме и голема вредност за max_depth, но од друга страна, ако ставиме премала вредност, нема да има простор добро да научи и ќе прави грешка дури и кај податоците што ги има видено. 
         
         
         
ЛАБС 7 СО ПИНГВИНИТЕ

      1. Издвојте ја колоната species во нова променлива. Потоа поделете го податочното множество на два дела. Поголемиот дел нека е 80% и ќе служи за тренирање, а помалиот дел од 20% ќе служи за проверка на повединието на алгоритмите кои ќе ги имплементирате понатаму.
      
         df_y = df['species']
df_x = df[['island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']]
from sklearn.model_selection import train_test_split
df_train_x, df_test_x, df_train_y, df_test_y = train_test_split(df_x, df_y, test_size=0.2, random_state=84)
df_test_x

  
       2. Исцртајте ги на график пингвините така што на x и y оските ќе биде големината на клунот (culmen), а класата на пингвините ќе биде претставена со боја или симбол.
Совет: За помош, искористите го примерот Setting size and color with column names од документацијата на плотли.

       import plotly.express as px
px.scatter(df, x="culmen_length_mm", y="culmen_depth_mm", color="species")


       3. Користејќи ја библиотеката sklearn, вчитајте невронска мрежа од типот Multi-layer Perceptron, претставено преку класата MLPClassifier, а потоа вметнете ги податоците за тренирање за да ја истренирате мрежата.
       
       df_x = df[['island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']]
df_x_encoded = pd.get_dummies(df_x, columns = ['island', 'sex'])
from sklearn.model_selection import train_test_split
df_y = df['species']
df_train_x, df_test_x, df_train_y, df_test_y = train_test_split(df_x_encoded, df_y, test_size=0.2, random_state=84)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

df_x_train_scaled = scaler.fit_transform(df_train_x)
df_x_test_scaled = scaler.transform(df_test_x)

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(
    hidden_layer_sizes=(50, ),
    activation='relu',
    solver='adam',
    max_iter=1000,
    random_state=84
)

mlp.fit(df_x_train_scaled, df_train_y)


       4. Пресметајте ја прецизноста на мрежата за податоците кои мрежата ги нема видено (на кои нема тренирано). Потоа пресметајте ја прецизноста на мрежата за податоците кои ги има видено (на кои има тренирано). Споредете ги резултатите. Ви изгледаат ли во ред?
       from sklearn.metrics import accuracy_score
accuracy_score(df_test_y, mlp.predict(df_x_test_scaled))
accuracy_score(df_train_y, mlp.predict(df_x_train_scaled))

      
       5. Кои пингвини мрежата успева да ги распознава подобро, машките или женските?
import numpy as np
from sklearn.metrics import accuracy_score

y_test_pred = mlp.predict(df_x_test_scaled)

test_data = df_test_x.copy()
test_data['true_species'] = df_test_y
test_data['pred_species'] = y_test_pred

male_mask = test_data['sex_MALE'] == 1

male_accuracy = accuracy_score(test_data.loc[male_mask, 'true_species'],test_data.loc[male_mask, 'pred_species'])

print(f"Preciznost na maski pingvini: {male_accuracy:.2f}")

female_mask = test_data['sex_FEMALE'] == 1

female_accuracy = accuracy_score(test_data.loc[female_mask, 'true_species'],test_data.loc[female_mask, 'pred_species'])

print(f"Preciznost na zenski pingvini: {female_accuracy:.2f}")


       6. Од кој вид е даден машки пингвин со признаци culmen_length_mm = 72, culmen_depth_mm = 34, flipper_length_mm = 202, body_mass_g = 4450, и е пронајден на островот Торгерсен?
       new_data = pd.DataFrame({
    'culmen_length_mm': [72],
    'culmen_depth_mm': [34],
    'flipper_length_mm': [202],
    'sex': ['MALE'],
    'body_mass_g': [4450],
    'island': ['Torgersen']
})

new_data_encoded = pd.get_dummies(new_data, columns=['island', 'sex'])

for col in df_x_encoded.columns:
    if col not in new_data_encoded.columns:
        new_data_encoded[col] = 0

new_data_encoded = new_data_encoded[df_x_encoded.columns]

new_data_scaled = scaler.transform(new_data_encoded)

predicted_species = mlp.predict(new_data_scaled)

print(f"Вид на пингвин: {predicted_species[0]}")




КАКО РАБОТИ МИНИМАКС АЛГОРИТАМОТ?

 - Алгоритмот Minimax работи врз принципот на минимизирање на можната загуба за најлошото сценарио. Претпоставува дека двајцата играчи играат оптимално. Алгоритмот рекурзивно ги проценува сите можни потези, конструирајќи дрво на игра каде што:

   Макс јазлите го претставуваат потегот на тековниот играч, со цел да се максимизира нивната предност.
   Мин јазлите го претставуваат потегот на противникот, со цел да се минимизира предноста на тековниот играч.

Ограничувања на Minimax

Иако е ефикасен, алгоритмот Minimax може да биде пресметковно интензивен, особено за игри кои што имаат многу состојби за разгранување како шах. Временската сложеност на Minimax е O(b^d), каде што b е факторот на разгранување, а d е длабочината на дрвото. Овој експоненцијален раст го прави непрактичен за длабински пребарувања без техники за оптимизација како што е Алфа-Бета поткрастување.


АЛГОРИТМИ ЗА УЧЕЊЕ СО НАДГЛЕДУВАЊЕ
    
    - 1. Чекор, 
         Множеството да се подели на податоци за тренирање и податоци за тест. Тоа се прави со train_test_split. Ги меша сите податоци и ги дели според test_size.

    - 2. Чекор,
         Ги даваме x_train и y_train на алгоритамот за учење
         
    - Gini impurity - функција многу слична на ентропијата. Изгледа слично и самата функција е без употреба на log. Користи множење кое е многу побрзо од пресметувањето на log, а во големи дрва на одлучување каде функцијата многу пати се повикува, многу време ќе се заштеди ако не се пресметува log.
    - Max_depth - некогаш имаме многу податоци, анализирањето на сите тие ќе земе многу повеќе време, многу меморија а може нема и да даде добар резултат. Затоа се поставува ова, да се разградува до одредена длабочина.
    - clf - classifier, се дефинира дрвото на одлулување
    - clf.fit - му ги праќаме x_train и y_train
    
    - 3 Чекор,
        clf.predict за да се добие prediction. Се спровдуваат со accuracy_score()
        
    - Многу е подобро алгоритамот да вади резултат што е поблизок и на train и на test. Ако на train вади 100% а на test 70%, тоа е overfitting.
      - Ако не добиеме вакво нешто, знаеме дека добро научил.
      - Како да не го преучиме?
       - Се дели множеството на x_train и y_train и x_test и y_test.
    
    
    
    
ДРВА НА ОДЛУЧУВАЊЕ

   - Една карактеристика го дели најдобро множеството. Како да се избере таа карактеристика?
     Најдобро го дели онаа карактеристика што има најголема информациска придобивка (според ID3 алгоритамот). К-ката со најдобра инф. придобивка се поставува за корен на дрвото.
   - Ако некоја од новите табели, по поделбата, има податоци со ист излез (перфектно класифицирани), во случајов на излез кога се доби за Rain, двете беа со Bus. Во тој случај        запираме со таа гранка. За сите останати што не се перфектно класифицирани, се продолжува истата постапка во наредна итерација.
   
   - Што претставува информациска придобивка?
     - Информациска придобивка е колку добро е поделено множеството за да даде успешна класификација.
     - Се добива како разлика помеѓу ентропијата пред поделбата на множеството и после поделбата на множеството. Бидејќи ентропијата по поделбата е на повеќе множества, се бара ентропија на секое од тие множества и потоа секое од нив се скалира со (бр. на членови на поделеното множество / вкупен бр. на членови) и на крај сите тие скалирани се собираат и така се одземаат од ентропијата пред поделбата. 
     - Информациската придобивка се проверува за секое Wheather, Time, Hungry, се гледа кое има подобра и така се одбира и затоа е алчен алгоритам.
    
   - Како се гледа подобра информациска придобивка?
     - Инф. прид. е разлика меѓу ентропијата пред и после поделбата. Ентропијата значи колку е во неред множеството. Па ако и после поделбата множеството е се уште во неред, разликата ќе биде мала, односно не е убаво распределено множеството, па нема да ја одбереме таа к-ка.
     
   - Ентропијата ни кажува колку добро е класифицирано множеството. Поголема ентропија значи по не во ред е множеството. (Ентропијата е мерка за неуредност.)
     Множество со скоро ист број на + и - е понеуредно и има поголема ентропија. Множество со 90% + или - е поуредно и има помала ентропија.
     
     
     

НАИВЕН БЕЈЕСОВ АЛГОРИТАМ
 
   - Зошто е наивен?
     Не го зема во предвид распоредот на зборовите како луѓето. Dear Friend = Friend Dear според него.
     Зборовите со наставки кои се однесуваат на истиот збор(пр. средина, средината), заради наставките, тој ги смета за различни зборови.
     Го третира јазикот како множество од зборови и секој од зборовите го гледа засебно. Затоа е невозможно или многу тешко и ќе одземе премногу време ако ги земеме во предвид сите можни правила.
     Смета дека секој признак е сам за себе.
     
   - Ако една од веројатностите R1 или R2, R3, R4 за некоја класа е 0, ќе се добие дека целата веројатност зборот да биде во таа класа е 0. За ова да се реши, тоа што не се паѓа ќе апроксимираме дека се паднало еднаш, при што се додава +1 на секоја к-ка и така се пресметува веројатноста. Ова се нарекува Лапласова корекција.
   
   - CountVectorizer - ф-ја што ги зима вредностите од еден текст и брои колку пати се имаат случено. Како таков податок се пракја на TfidfTransformer.
   
   - TfidfTransformer - за секој збор се запишува фреквенцијата со која се случува. 
     На пр. Во еден текст имаме вкупно 250 зборови, а зборот "на" се повторува 10 пати (добиено од CountVectorizer), па фреквенцијата ќе биде 10/250 или 1/25. Ова се праќа на влез на MultinomialNB.
   
   - MultinomialNB (сместено е во sckitlearn) - има аргумент алфа што ја прави лапласовата корекција.
   
   - Ако сакаме да видиме каде има згрешено/која класа ја промашил, се пишува zip(test_labels, y_pred).
     - test_labels се вредностите што ние ги имаме
     - y_pred се вредностите што алгоритамот ги предвидува за test_dataset
     - се итерира со:
       
       for true, pred in zip(test_labels, y_pred):
          if true!=pred:
            print(true, pred)
            
     - ако сакаме да го видиме и текстот каде ја направил грешката: 
     
       for test, true, pred in zip(test_dataset, test_labels, y_pred):
           if true!=pred:
              print(true, '-', pred)
              print(text)
              print('-----------------------------------')
              
     - zip ги спојува сите податоци во еден текст.
     
     
     - Како да се подобри?
      1. Да се тргнат наставките зошто алгоритамот зборовите со и без наставка ги смета за различен збор.
      2. Да се решат проблемите каде два зборови се спојуваат во еден
      3. Да се отстранат сврзниците бидејќи ги има во поголем број и алгоритамот троши време за да ги пресмета а немаат никакво друго влијание во алгоритамот и не може да помогнат во полесно класифицирање на текстот.
      
     - Самостојна задача, да се имплементира лапласова корекција во кодот без sckitlearn: 
        Треба само овој код да се имплементира
        def r(feature_x_index, value, output_class):        
            a = sum([output_class == row[-1] for row in table])
	    b = sum([output_class == row[-1] and row[feature_x_index-1] == value for row in table]) 
	    return (b+1) / (a+2)

	r(2, 0, 1)
      
         
   
     
     
  

